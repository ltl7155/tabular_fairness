{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ebf9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "layer6 (Dense)               (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,736\n",
      "Trainable params: 1,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 1\n",
      "attr: a layer: 0 diff: 0.269\n",
      "layer 2\n",
      "attr: a layer: 1 diff: 0.296\n",
      "layer 3\n",
      "attr: a layer: 2 diff: 0.138\n",
      "layer 4\n",
      "attr: a layer: 3 diff: 0.273\n",
      "layer 5\n",
      "attr: a layer: 4 diff: 0.17\n",
      "layer 6\n",
      "attr: a layer: 5 diff: 0.138\n",
      "layer 1\n",
      "attr: r layer: 0 diff: 0.275\n",
      "layer 2\n",
      "attr: r layer: 1 diff: 0.237\n",
      "layer 3\n",
      "attr: r layer: 2 diff: 0.152\n",
      "layer 4\n",
      "attr: r layer: 3 diff: 0.233\n",
      "layer 5\n",
      "attr: r layer: 4 diff: 0.15\n",
      "layer 6\n",
      "attr: r layer: 5 diff: 0.106\n",
      "layer 1\n",
      "attr: g layer: 0 diff: 0.126\n",
      "layer 2\n",
      "attr: g layer: 1 diff: 0.135\n",
      "layer 3\n",
      "attr: g layer: 2 diff: 0.091\n",
      "layer 4\n",
      "attr: g layer: 3 diff: 0.169\n",
      "layer 5\n",
      "attr: g layer: 4 diff: 0.112\n",
      "layer 6\n",
      "attr: g layer: 5 diff: 0.082\n",
      "layer 1\n",
      "attr: a&r layer: 0 diff: 0.459\n",
      "layer 2\n",
      "attr: a&r layer: 1 diff: 0.393\n",
      "layer 3\n",
      "attr: a&r layer: 2 diff: 0.169\n",
      "layer 4\n",
      "attr: a&r layer: 3 diff: 0.145\n",
      "layer 5\n",
      "attr: a&r layer: 4 diff: 0.093\n",
      "layer 6\n",
      "attr: a&r layer: 5 diff: 0.141\n",
      "layer 1\n",
      "attr: a&g layer: 0 diff: 0.341\n",
      "layer 2\n",
      "attr: a&g layer: 1 diff: 0.368\n",
      "layer 3\n",
      "attr: a&g layer: 2 diff: 0.154\n",
      "layer 4\n",
      "attr: a&g layer: 3 diff: 0.24\n",
      "layer 5\n",
      "attr: a&g layer: 4 diff: 0.149\n",
      "layer 6\n",
      "attr: a&g layer: 5 diff: 0.138\n",
      "layer 1\n",
      "attr: r&g layer: 0 diff: 0.329\n",
      "layer 2\n",
      "attr: r&g layer: 1 diff: 0.289\n",
      "layer 3\n",
      "attr: r&g layer: 2 diff: 0.175\n",
      "layer 4\n",
      "attr: r&g layer: 3 diff: 0.162\n",
      "layer 5\n",
      "attr: r&g layer: 4 diff: 0.099\n",
      "layer 6\n",
      "attr: r&g layer: 5 diff: 0.112\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_census_income\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=pre_census_income.X_test.shape[1:]),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            'r': [6],\n",
    "            'g': [7],\n",
    "            'a&r': [0, 6],\n",
    "            'a&g': [0, 7],\n",
    "            'r&g': [6, 7]\n",
    "            }\n",
    "\n",
    "model_path = \"models/adult_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "# print(model.get_layer('layer6').get_weights())\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('layer1', 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'scale_layer_8'), ('layer6', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_census_income\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for attr in pos_map.keys():\n",
    "    for layer_index in range(0, 6):\n",
    "        print(\"layer\", layer_index+1) \n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        data_name = f\"discriminatory_data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "        dis_data = np.load(data_name)\n",
    "\n",
    "#         dis_data = pre_census_income.X_test\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_census_income.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(dis_data)\n",
    "\n",
    "        max_v = inter_output_ori.max()\n",
    "        min_v = inter_output_ori.min()\n",
    "        \n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum() / (max_v - min_v)\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, \"layer:\", layer_index, \"diff:\", round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a8167d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 30)                510       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,856\n",
      "Trainable params: 1,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 0\n",
      "attr: a layer: 0 diff: 0.485\n",
      "layer 1\n",
      "attr: a layer: 1 diff: 0.433\n",
      "layer 2\n",
      "attr: a layer: 2 diff: 0.26\n",
      "layer 3\n",
      "attr: a layer: 3 diff: 0.246\n",
      "layer 4\n",
      "attr: a layer: 4 diff: 0.24\n",
      "layer 5\n",
      "attr: a layer: 5 diff: 0.12\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_bank_marketing\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=pre_bank_marketing.X_test.shape[1:]),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            }\n",
    "\n",
    "model_path = \"models/bank_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('layer1', 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'scale_layer_8'), ('dense_137', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_bank_marketing\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for attr in pos_map.keys():\n",
    "    for layer_index in range(0,6):\n",
    "        print(\"layer\", layer_index)\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        data_name = f\"discriminatory_data/bank/B-{attr}_ids_EIDIG_INF.npy\"\n",
    "        dis_data = np.load(data_name)\n",
    "\n",
    "#         dis_data = pre_bank_marketing.X_test\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_bank_marketing.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(dis_data)\n",
    "        max_v = inter_output_ori.max()\n",
    "        min_v = inter_output_ori.min()\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum() / (max_v - min_v)\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, \"layer:\", layer_index, \"diff:\", round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d445c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "[array([[ 0.8240238 ],\n",
      "       [-0.26753247],\n",
      "       [-0.0609949 ],\n",
      "       [-0.5809436 ],\n",
      "       [ 0.49730486],\n",
      "       [-0.3065411 ],\n",
      "       [-0.23086877],\n",
      "       [ 0.40636757],\n",
      "       [-0.5609462 ],\n",
      "       [ 0.29439944]], dtype=float32), array([-0.43622154], dtype=float32)]\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,736\n",
      "Trainable params: 1,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 1\n",
      "attr: a layer: 0 diff: 0.139\n",
      "layer 2\n",
      "attr: a layer: 1 diff: 0.157\n",
      "layer 3\n",
      "attr: a layer: 2 diff: 0.069\n",
      "layer 4\n",
      "attr: a layer: 3 diff: 0.055\n",
      "layer 5\n",
      "attr: a layer: 4 diff: 0.017\n",
      "layer 6\n",
      "attr: a layer: 5 diff: 0.021\n",
      "layer 1\n",
      "attr: r layer: 0 diff: 0.067\n",
      "layer 2\n",
      "attr: r layer: 1 diff: 0.063\n",
      "layer 3\n",
      "attr: r layer: 2 diff: 0.039\n",
      "layer 4\n",
      "attr: r layer: 3 diff: 0.032\n",
      "layer 5\n",
      "attr: r layer: 4 diff: 0.016\n",
      "layer 6\n",
      "attr: r layer: 5 diff: 0.011\n",
      "layer 1\n",
      "attr: g layer: 0 diff: 0.056\n",
      "layer 2\n",
      "attr: g layer: 1 diff: 0.055\n",
      "layer 3\n",
      "attr: g layer: 2 diff: 0.033\n",
      "layer 4\n",
      "attr: g layer: 3 diff: 0.025\n",
      "layer 5\n",
      "attr: g layer: 4 diff: 0.011\n",
      "layer 6\n",
      "attr: g layer: 5 diff: 0.008\n",
      "layer 1\n",
      "attr: a&r layer: 0 diff: 0.173\n",
      "layer 2\n",
      "attr: a&r layer: 1 diff: 0.186\n",
      "layer 3\n",
      "attr: a&r layer: 2 diff: 0.097\n",
      "layer 4\n",
      "attr: a&r layer: 3 diff: 0.069\n",
      "layer 5\n",
      "attr: a&r layer: 4 diff: 0.03\n",
      "layer 6\n",
      "attr: a&r layer: 5 diff: 0.029\n",
      "layer 1\n",
      "attr: a&g layer: 0 diff: 0.168\n",
      "layer 2\n",
      "attr: a&g layer: 1 diff: 0.169\n",
      "layer 3\n",
      "attr: a&g layer: 2 diff: 0.076\n",
      "layer 4\n",
      "attr: a&g layer: 3 diff: 0.061\n",
      "layer 5\n",
      "attr: a&g layer: 4 diff: 0.028\n",
      "layer 6\n",
      "attr: a&g layer: 5 diff: 0.025\n",
      "layer 1\n",
      "attr: r&g layer: 0 diff: 0.095\n",
      "layer 2\n",
      "attr: r&g layer: 1 diff: 0.085\n",
      "layer 3\n",
      "attr: r&g layer: 2 diff: 0.055\n",
      "layer 4\n",
      "attr: r&g layer: 3 diff: 0.036\n",
      "layer 5\n",
      "attr: r&g layer: 4 diff: 0.015\n",
      "layer 6\n",
      "attr: r&g layer: 5 diff: 0.016\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_census_income\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=pre_census_income.X_test.shape[1:]),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            'r': [6],\n",
    "            'g': [7],\n",
    "            'a&r': [0, 6],\n",
    "            'a&g': [0, 7],\n",
    "            'r&g': [6, 7]\n",
    "            }\n",
    "\n",
    "model_path = \"models/retrained_model_EIDIG/adult_EIDIG_INF_retrained_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "print(model.get_layer('dense_5').get_weights())\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('layer1', 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'scale_layer_8'), ('dense_5', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_census_income\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for attr in pos_map.keys():\n",
    "    for layer_index in range(0, 6):\n",
    "        print(\"layer\", layer_index+1)\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        data_name = f\"discriminatory_data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "        dis_data = np.load(data_name)\n",
    "\n",
    "#         dis_data = pre_census_income.X_test\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_census_income.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "#         inter_output_ori = inter_model.predict(pre_census_income.X_test)\n",
    "        inter_output_ori = inter_model.predict(dis_data)\n",
    "        max_v = inter_output_ori.max()\n",
    "        min_v = inter_output_ori.min()\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum() / (max_v - min_v)\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, \"layer:\", layer_index, \"diff:\", round(diff/num, 3))\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8df82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 30)                510       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 1,666\n",
      "Trainable params: 1,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 0\n",
      "attr: a layer: 0 diff: 0.3\n",
      "layer 1\n",
      "attr: a layer: 1 diff: 0.191\n",
      "layer 2\n",
      "attr: a layer: 2 diff: 0.092\n",
      "layer 3\n",
      "attr: a layer: 3 diff: 0.041\n",
      "layer 4\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_bank_marketing\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=pre_bank_marketing.X_train.shape[1:]),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            }\n",
    "\n",
    "model_path = \"models/retrained_model_EIDIG/bank_EIDIG_INF_retrained_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [(\"layer1\", 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'scale_layer_8'), ('dense_17', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_bank_marketing\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for attr in pos_map.keys():\n",
    "    for layer_index in range(0, 6):\n",
    "        print(\"layer\", layer_index)\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        data_name = f\"discriminatory_data/bank/B-{attr}_ids_EIDIG_INF.npy\"\n",
    "        dis_data = np.load(data_name)\n",
    "\n",
    "#         dis_data = pre_bank_marketing.X_train\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_bank_marketing.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(dis_data)\n",
    "        max_v = inter_output_ori.max()\n",
    "        min_v = inter_output_ori.min()\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum() / (max_v - min_v)\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, \"layer:\", layer_index, \"diff:\", round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b92fc285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 50)                600       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 2,816\n",
      "Trainable params: 2,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 0\n",
      "attr: r layer: 0 diff: 0.339\n",
      "layer 1\n",
      "attr: r layer: 1 diff: 0.141\n",
      "layer 2\n",
      "attr: r layer: 2 diff: 0.22\n",
      "layer 3\n",
      "attr: r layer: 3 diff: 0.317\n",
      "layer 4\n",
      "attr: r layer: 4 diff: 0.198\n",
      "layer 5\n",
      "attr: r layer: 5 diff: 0.178\n",
      "layer 0\n",
      "attr: g layer: 0 diff: 0.151\n",
      "layer 1\n",
      "attr: g layer: 1 diff: 0.047\n",
      "layer 2\n",
      "attr: g layer: 2 diff: 0.077\n",
      "layer 3\n",
      "attr: g layer: 3 diff: 0.121\n",
      "layer 4\n",
      "attr: g layer: 4 diff: 0.084\n",
      "layer 5\n",
      "attr: g layer: 5 diff: 0.089\n",
      "layer 0\n",
      "attr: g&r layer: 0 diff: 0.458\n",
      "layer 1\n",
      "attr: g&r layer: 1 diff: 0.169\n",
      "layer 2\n",
      "attr: g&r layer: 2 diff: 0.231\n",
      "layer 3\n",
      "attr: g&r layer: 3 diff: 0.356\n",
      "layer 4\n",
      "attr: g&r layer: 4 diff: 0.18\n",
      "layer 5\n",
      "attr: g&r layer: 5 diff: 0.165\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_lsac\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(50, activation=\"relu\", input_shape=pre_lsac.X_train.shape[1:]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(5, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(int(constraint[i][0]), int(constraint[i][1]+1)))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { \n",
    "            'r': [10],\n",
    "            'g': [9],\n",
    "            'g&r': [10, 9]\n",
    "            }\n",
    "\n",
    "model_path = \"models/lsac_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('layer1', 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'scale_layer_8'), ('dense_5', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_lsac\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for attr in pos_map.keys():\n",
    "    for layer_index in range(0, 6):\n",
    "        print(\"layer\", layer_index)\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        data_name = f\"discriminatory_data/lsac/lsac-{attr}_ids_EIDIG_INF_1.npy\"\n",
    "        dis_data = np.load(data_name)\n",
    "\n",
    "#         dis_data = pre_lsac.X_train\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_lsac.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(dis_data)\n",
    "        max_v = inter_output_ori.max()\n",
    "        min_v = inter_output_ori.min()\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum() / (max_v - min_v)\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, \"layer:\", layer_index, \"diff:\", round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c19fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 50)                600       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 2,816\n",
      "Trainable params: 2,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 0\n",
      "attr: r layer: 0 diff: 0.383\n",
      "layer 1\n",
      "attr: r layer: 1 diff: 0.177\n",
      "layer 2\n",
      "attr: r layer: 2 diff: 0.066\n",
      "layer 3\n",
      "attr: r layer: 3 diff: 0.05\n",
      "layer 4\n",
      "attr: r layer: 4 diff: 0.05\n",
      "layer 5\n",
      "attr: r layer: 5 diff: 0.036\n",
      "layer 0\n",
      "attr: g layer: 0 diff: 0.18\n",
      "layer 1\n",
      "attr: g layer: 1 diff: 0.111\n",
      "layer 2\n",
      "attr: g layer: 2 diff: 0.037\n",
      "layer 3\n",
      "attr: g layer: 3 diff: 0.027\n",
      "layer 4\n",
      "attr: g layer: 4 diff: 0.026\n",
      "layer 5\n",
      "attr: g layer: 5 diff: 0.017\n",
      "layer 0\n",
      "attr: g&r layer: 0 diff: 0.51\n",
      "layer 1\n",
      "attr: g&r layer: 1 diff: 0.22\n",
      "layer 2\n",
      "attr: g&r layer: 2 diff: 0.101\n",
      "layer 3\n",
      "attr: g&r layer: 3 diff: 0.067\n",
      "layer 4\n",
      "attr: g&r layer: 4 diff: 0.064\n",
      "layer 5\n",
      "attr: g&r layer: 5 diff: 0.047\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_lsac\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(50, activation=\"relu\", input_shape=pre_lsac.X_train.shape[1:]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(5, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(int(constraint[i][0]), int(constraint[i][1]+1)))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { \n",
    "            'r': [10],\n",
    "            'g': [9],\n",
    "            'g&r': [10, 9]\n",
    "            }\n",
    "\n",
    "model_path = \"models/retrained_model_EIDIG/lsac_EIDIG_INF_retrained_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = tf.keras.models.load_model(model_path, compile=False)\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('layer1', 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'scale_layer_8'), ('dense_23', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_lsac\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for attr in pos_map.keys():\n",
    "    for layer_index in range(0, 6):\n",
    "        print(\"layer\", layer_index)\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        data_name = f\"discriminatory_data/lsac/lsac-{attr}_ids_EIDIG_INF_1.npy\"\n",
    "        dis_data = np.load(data_name)\n",
    "\n",
    "#         dis_data = pre_lsac.X_train\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_lsac.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(dis_data)\n",
    "        max_v = inter_output_ori.max()\n",
    "        min_v = inter_output_ori.min()\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum() / (max_v - min_v)\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, \"layer:\", layer_index, \"diff:\", round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebf6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080ae51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda47d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
