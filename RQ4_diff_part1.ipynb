{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82ebf9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "layer6 (Dense)               (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,736\n",
      "Trainable params: 1,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 5\n",
      "attr: a 0.17\n",
      "attr: r 0.15\n",
      "attr: g 0.112\n",
      "attr: a&r 0.093\n",
      "attr: a&g 0.149\n",
      "attr: r&g 0.099\n",
      "layer 6\n",
      "attr: a 0.138\n",
      "attr: r 0.106\n",
      "attr: g 0.082\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a2bfe1d01cfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0minter_output_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minter_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minter_output_adv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minter_output_ori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_v\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_census_income\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=pre_census_income.X_test.shape[1:]),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            'r': [6],\n",
    "            'g': [7],\n",
    "            'a&r': [0, 6],\n",
    "            'a&g': [0, 7],\n",
    "            'r&g': [6, 7]\n",
    "            }\n",
    "\n",
    "model_path = \"models/adult_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "# print(model.get_layer('layer6').get_weights())\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('layer1', 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'scale_layer_8'), ('layer6', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_census_income\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for layer_index in range(0, 6):\n",
    "    print(\"layer\", layer_index+1)\n",
    "    for attr in pos_map.keys():\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        data_name = f\"discriminatory_data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "        dis_data = np.load(data_name)\n",
    "\n",
    "#         dis_data = pre_census_income.X_test\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_census_income.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(dis_data)\n",
    "\n",
    "        max_v = inter_output_ori.max()\n",
    "        min_v = inter_output_ori.min()\n",
    "        \n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum() / (max_v - min_v)\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, \"layer:\", layer_index, \"diff:\", round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a8167d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xiaofei/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 30)                510       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,856\n",
      "Trainable params: 1,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 0\n",
      "attr: a 0.493\n",
      "layer 1\n",
      "attr: a 0.394\n",
      "layer 2\n",
      "attr: a 0.196\n",
      "layer 3\n",
      "attr: a 0.115\n",
      "layer 4\n",
      "attr: a 0.078\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_bank_marketing\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=pre_bank_marketing.X_test.shape[1:]),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            }\n",
    "\n",
    "model_path = \"models/bank_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('layer1', 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'scale_layer_8'), ('dense_137', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_bank_marketing\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for attr in pos_map.keys():\n",
    "    for layer_index in range(0,6):\n",
    "        print(\"layer\", layer_index)\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        data_name = f\"data/bank/B-{attr}_ids_EIDIG_INF.npy\"\n",
    "        dis_data = np.load(data_name)\n",
    "\n",
    "#         dis_data = pre_bank_marketing.X_test\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_bank_marketing.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(dis_data)\n",
    "        max_v = inter_output_ori.max()\n",
    "        min_v = inter_output_ori.min()\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum() / (max_v - min_v)\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, \"layer:\", layer_index, \"diff:\", round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d445c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_census_income\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=pre_census_income.X_test.shape[1:]),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            'r': [6],\n",
    "            'g': [7],\n",
    "            'a&r': [0, 6],\n",
    "            'a&g': [0, 7],\n",
    "            'r&g': [6, 7]\n",
    "            }\n",
    "\n",
    "model_path = \"models/retrained_model_EIDIG/adult_EIDIG_INF_retrained_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "print(model.get_layer('dense_5').get_weights())\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('layer1', 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'scale_layer_8'), ('dense_5', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_census_income\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for attr in pos_map.keys():\n",
    "    for layer_index in range(0, 6):\n",
    "        print(\"layer\", layer_index+1)\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        data_name = f\"discriminatory_data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "        dis_data = np.load(data_name)\n",
    "\n",
    "#         dis_data = pre_census_income.X_test\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_census_income.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "#         inter_output_ori = inter_model.predict(pre_census_income.X_test)\n",
    "        inter_output_ori = inter_model.predict(dis_data)\n",
    "        max_v = inter_output_ori.max()\n",
    "        min_v = inter_output_ori.min()\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum() / (max_v - min_v)\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, \"layer:\", layer_index, \"diff:\", round(diff/num, 3))\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061c5fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 30)                510       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 1,666\n",
      "Trainable params: 1,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 0\n",
      "attr: a 0.356\n",
      "layer 1\n",
      "attr: a 0.184\n",
      "layer 2\n",
      "attr: a 0.061\n",
      "layer 3\n",
      "attr: a 0.023\n",
      "layer 4\n",
      "attr: a 0.012\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_bank_marketing\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=pre_bank_marketing.X_train.shape[1:]),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            }\n",
    "\n",
    "model_path = \"models/retrained_model_EIDIG/bank_EIDIG_INF_retrained_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [(\"layer1\", 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'scale_layer_8'), ('dense_17', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_bank_marketing\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for attr in pos_map.keys():\n",
    "    for layer_index in range(5):\n",
    "        print(\"layer\", layer_index)\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        data_name = f\"discriminatory_data/bank/B-{attr}_ids_EIDIG_INF.npy\"\n",
    "        dis_data = np.load(data_name)\n",
    "\n",
    "#         dis_data = pre_bank_marketing.X_train\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_bank_marketing.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(dis_data)\n",
    "        max_v = inter_output_ori.max()\n",
    "        min_v = inter_output_ori.min()\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum() / (max_v - min_v)\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, \"layer:\", layer_index, \"diff:\", round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b92fc285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 50)                600       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 2,816\n",
      "Trainable params: 2,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 0\n",
      "attr: r 0.494\n",
      "attr: g 0.239\n",
      "attr: g&r 0.614\n",
      "layer 1\n",
      "attr: r 0.163\n",
      "attr: g 0.055\n",
      "attr: g&r 0.186\n",
      "layer 2\n",
      "attr: r 0.145\n",
      "attr: g 0.054\n",
      "attr: g&r 0.167\n",
      "layer 3\n",
      "attr: r 0.116\n",
      "attr: g 0.058\n",
      "attr: g&r 0.142\n",
      "layer 4\n",
      "attr: r 0.061\n",
      "attr: g 0.025\n",
      "attr: g&r 0.072\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_lsac\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(50, activation=\"relu\", input_shape=pre_lsac.X_train.shape[1:]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(5, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(int(constraint[i][0]), int(constraint[i][1]+1)))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { \n",
    "            'r': [10],\n",
    "            'g': [9],\n",
    "            'g&r': [10, 9]\n",
    "            }\n",
    "\n",
    "model_path = \"models/lsac_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('layer1', 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_lsac\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for attr in pos_map.keys():\n",
    "    for layer_index in range(5):\n",
    "        print(\"layer\", layer_index)\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        data_name = f\"discriminatory_data/lsac/lsac-{attr}_ids_EIDIG_INF_1.npy\"\n",
    "        dis_data = np.load(data_name)\n",
    "\n",
    "#         dis_data = pre_lsac.X_train\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_lsac.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(dis_data)\n",
    "        max_v = inter_output_ori.max()\n",
    "        min_v = inter_output_ori.min()\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum() / (max_v - min_v)\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, \"layer:\", layer_index, \"diff:\", round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c19fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 50)                600       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 2,816\n",
      "Trainable params: 2,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 0\n",
      "attr: r 0.218\n",
      "attr: g 0.108\n",
      "attr: g&r 0.274\n",
      "layer 1\n",
      "attr: r 0.086\n",
      "attr: g 0.042\n",
      "attr: g&r 0.106\n",
      "layer 2\n",
      "attr: r 0.067\n",
      "attr: g 0.031\n",
      "attr: g&r 0.083\n",
      "layer 3\n",
      "attr: r 0.047\n",
      "attr: g 0.023\n",
      "attr: g&r 0.058\n",
      "layer 4\n",
      "attr: r 0.047\n",
      "attr: g 0.023\n",
      "attr: g&r 0.059\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_lsac\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(50, activation=\"relu\", input_shape=pre_lsac.X_train.shape[1:]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(5, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(int(constraint[i][0]), int(constraint[i][1]+1)))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { \n",
    "            'r': [10],\n",
    "            'g': [9],\n",
    "            'g&r': [10, 9]\n",
    "            }\n",
    "\n",
    "model_path = \"models/retrained_model_EIDIG/lsac_EIDIG_INF_retrained_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = tf.keras.models.load_model(model_path, compile=False)\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('layer1', 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_lsac\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for attr in pos_map.keys():\n",
    "    for layer_index in range(5):\n",
    "        print(\"layer\", layer_index)\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        data_name = f\"discriminatory_data/lsac/lsac-{attr}_ids_EIDIG_INF_1.npy\"\n",
    "        dis_data = np.load(data_name)\n",
    "\n",
    "#         dis_data = pre_lsac.X_train\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_lsac.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(dis_data)\n",
    "        max_v = inter_output_ori.max()\n",
    "        min_v = inter_output_ori.min()\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum() / (max_v - min_v)\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, \"layer:\", layer_index, \"diff:\", round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebf6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080ae51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda47d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
