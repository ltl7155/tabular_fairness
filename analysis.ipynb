{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_census_income\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "class ScaleLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dense_len, min=-1, max=1, **kwargs):\n",
    "        super(ScaleLayer, self).__init__(**kwargs)\n",
    "        tf.keras.constraints.MinMaxNorm()\n",
    "        self.scale = K.variable([[1. for x in range(dense_len)]], name='ffff',\n",
    "                                constraint=lambda t: tf.clip_by_value(t, min, max))\n",
    "        self.dense_len = dense_len\n",
    "    def call(self, inputs, **kwargs):\n",
    "        m = inputs * self.scale\n",
    "        return m\n",
    "    def get_config(self):\n",
    "        config = {'dense_len': self.dense_len}\n",
    "        base_config = super(ScaleLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "def construct_model(neurons, top_layer, name, min, max, need_weights=True):\n",
    "    in_shape = X_train.shape[1:]\n",
    "    input = keras.Input(shape=in_shape)\n",
    "    layer1 = keras.layers.Dense(30, name=\"layer1\")\n",
    "    d1 = ScaleLayer(30, min, max)\n",
    "    layer2 = keras.layers.Dense(20, name=\"layer2\")\n",
    "    d2 = ScaleLayer(20, min, max)\n",
    "    layer3 = keras.layers.Dense(15, name=\"layer3\")\n",
    "    d3 = ScaleLayer(15, min, max)\n",
    "    layer4 = keras.layers.Dense(15, name=\"layer4\")\n",
    "    d4 = ScaleLayer(15, min, max)\n",
    "    layer5 = keras.layers.Dense(10,name=\"layer5\")\n",
    "    d5 = ScaleLayer(10, min, max)\n",
    "    layer6 = keras.layers.Dense(1, activation=\"sigmoid\", name=\"layer6\")\n",
    "\n",
    "    layer_lst = [layer1, layer2, layer3, layer4, layer5]\n",
    "    ds = [d1, d2, d3, d4, d5]\n",
    "    for layer in layer_lst[0: top_layer]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = input\n",
    "    for i, l in enumerate(layer_lst):\n",
    "        x = l(x)\n",
    "        if i < top_layer:\n",
    "            x = ds[i](x)\n",
    "    x = layer6(x)\n",
    "\n",
    "    if not need_weights:\n",
    "        return keras.Model(input, x)\n",
    "\n",
    "    w = 0.\n",
    "    for i, re in enumerate(neurons):\n",
    "        neg = re[0]\n",
    "        pos = re[1]\n",
    "        d = ds[i]\n",
    "        for m in neg:\n",
    "            w = tf.math.add(w, d.weights[0][0][m])\n",
    "        for n in pos:\n",
    "            w = tf.math.subtract(w, d.weights[0][0][n])\n",
    "    new_w = tf.identity(tf.reshape(w, [1, 1]), name=name)\n",
    "\n",
    "    model = keras.Model(input, [x, new_w])\n",
    "    return model\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            'r': [6],\n",
    "            'g': [7],\n",
    "            'a&r': [0, 6],\n",
    "            'a&g': [0, 7],\n",
    "            'r&g': [6, 7]\n",
    "            }\n",
    "\n",
    "attr = 'g'\n",
    "protected_attribs = pos_map[attr]\n",
    "\n",
    "data_name = f\"data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "# dis_data = np.load(data_name)\n",
    "\n",
    "dis_data = pre_census_income.X_train\n",
    "num_attribs = len(dis_data[0])\n",
    "new_data = dis_data.copy()\n",
    "new_data[:, 7] = 1 - dis_data[:, 7]\n",
    "\n",
    "similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_census_income.constraint)\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('layer1', 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'scale_layer_8'), ('layer6', 'scale_layer_8')]\n",
    "\n",
    "layer_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 12)]              0         \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "scale_layer_5 (ScaleLayer)   (None, 30)                30        \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "scale_layer_6 (ScaleLayer)   (None, 20)                20        \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "scale_layer_7 (ScaleLayer)   (None, 15)                15        \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "scale_layer_8 (ScaleLayer)   (None, 15)                15        \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "layer6 (Dense)               (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,816\n",
      "Trainable params: 1,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "368.91714\n",
      "188515.02\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import pre_census_income\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model_path = \"models/diff_adult_g_gated_4_diff.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path, custom_objects={'ScaleLayer': ScaleLayer})\n",
    "model.summary()\n",
    "\n",
    "layer_name = layer_map[layer_index][1]\n",
    "inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "                                 \n",
    "layer_name = layer_map[layer_index][0]\n",
    "inter_model_before = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "                                 \n",
    "inter_output_ori = inter_model.predict(similar_X[0])\n",
    "inter_output_adv = inter_model.predict(similar_X[1])\n",
    "\n",
    "inter_output_ori_before = inter_model_before.predict(similar_X[0])\n",
    "inter_output_adv_before = inter_model_before.predict(similar_X[1])\n",
    "                                 \n",
    "print((np.abs(inter_output_adv - inter_output_ori)).sum())\n",
    "print((np.abs(inter_output_adv_before - inter_output_ori_before)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight [array([[ 7.5974339e-04, -5.3571258e-04, -5.2292697e-04, -8.3338644e-05,\n",
      "         1.9389287e-03,  2.4732639e-04, -4.9206632e-05,  2.5967529e-04,\n",
      "        -1.2399700e-01,  6.0464849e-04,  9.1479821e-03,  2.2583730e-04,\n",
      "         2.8091979e-03,  1.4539480e-03,  6.5577053e-04, -1.7852495e-04,\n",
      "         9.0948762e-03, -7.5216143e-05,  8.5201718e-05, -6.2989420e-04,\n",
      "         1.3095517e-02, -2.0224368e-04, -1.3083528e-03, -2.0436089e-02,\n",
      "         9.8191772e-04, -2.0736131e-04, -2.0158633e-03, -1.9681137e-03,\n",
      "        -8.8466989e-04,  4.5708522e-02]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "weight= model.get_layer('scale_layer_5').get_weights()\n",
    "print(\"weight\", weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': ['models/finetuned_models_protected_attributes/adult/r_adult_model_1_0.986.h5', 'models/finetuned_models_protected_attributes/adult/r_adult_model_2_0.946.h5', 'models/finetuned_models_protected_attributes/adult/r_adult_model_3_0.881.h5', 'models/finetuned_models_protected_attributes/adult/r_adult_model_4_0.871.h5', 'models/finetuned_models_protected_attributes/adult/r_adult_model_5_0.859.h5'], 'g': ['models/finetuned_models_protected_attributes/adult/g_adult_model_1_0.997.h5', 'models/finetuned_models_protected_attributes/adult/g_adult_model_2_0.968.h5', 'models/finetuned_models_protected_attributes/adult/g_adult_model_3_0.848.h5', 'models/finetuned_models_protected_attributes/adult/g_adult_model_4_0.826.h5', 'models/finetuned_models_protected_attributes/adult/g_adult_model_5_0.768.h5'], 'a': ['models/finetuned_models_protected_attributes/adult/a_adult_model_1_0.994.h5', 'models/finetuned_models_protected_attributes/adult/a_adult_model_2_0.965.h5', 'models/finetuned_models_protected_attributes/adult/a_adult_model_3_0.771.h5', 'models/finetuned_models_protected_attributes/adult/a_adult_model_4_0.705.h5', 'models/finetuned_models_protected_attributes/adult/a_adult_model_5_0.629.h5']}\n",
      "current_penalty [7 9] current_awarded []\n",
      "current_penalty [ 0  2  3  8 24 25 27] current_awarded [ 7 19 28]\n",
      "current_penalty [ 8 10 16 18] current_awarded [5 6]\n",
      "current_penalty [4] current_awarded [ 7  9 10]\n",
      "current_penalty [0 2 5 6 8] current_awarded [ 4 10]\n",
      "current_penalty [6 8 9] current_awarded [2 3]\n",
      "current_penalty [] current_awarded []\n",
      "current_penalty [] current_awarded []\n",
      "4.62468991960798\n",
      "1.3163425922393799\n",
      "16.62976989746094\n",
      "penalty [ 0  2  3  8 24 25 27]\n",
      "awarded [ 7 19 28]\n",
      "normal [1, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 26, 29]\n",
      "368.91714\n"
     ]
    }
   ],
   "source": [
    "from explain import  get_relevance, get_critical_neurons\n",
    "\n",
    "def my_filter(layer_critical, total_num):\n",
    "    i_unique, i_counts = np.unique(layer_critical, return_counts=True)\n",
    "    i_rates = i_counts / total_num\n",
    "    i_sort = np.where(i_rates > 0.2)[0]  # np.argsort(i_counts*-1)\n",
    "    i_critical = i_unique[i_sort]\n",
    "    return i_critical\n",
    "\n",
    "def get_path_dict():\n",
    "    saved_model_path = \"models/finetuned_models_protected_attributes/adult/\"\n",
    "    path_ls = os.listdir(saved_model_path)\n",
    "    path_dict = {}\n",
    "    path_dict['r'] = [saved_model_path+p for p in path_ls if \"r_adult\" in p]\n",
    "    path_dict['g'] = [saved_model_path+p for p in path_ls if \"g_adult\" in p]\n",
    "    path_dict['a'] = [saved_model_path+p for p in path_ls if \"a_adult\" in p]\n",
    "    path_dict['r'].sort()\n",
    "    path_dict['g'].sort()\n",
    "    path_dict['a'].sort()\n",
    "    print(path_dict)\n",
    "    return path_dict\n",
    "\n",
    "\n",
    "def get_penalty_awarded(top_n, layer_num, total_num, income_critical, protected_critical_ls):\n",
    "    neurons = []\n",
    "\n",
    "    for i in range(layer_num):\n",
    "        income_layer_critical = income_critical[i].flatten()\n",
    "        i_critical = my_filter(income_layer_critical, total_num)\n",
    "        current_penalty = None\n",
    "        current_awarded = None\n",
    "        filtered_criticals = []\n",
    "        j = 0\n",
    "        a = 'g'\n",
    "        protected_layer_critical = protected_critical_ls[j][i].flatten()\n",
    "        p_critical = my_filter(protected_layer_critical, total_num)\n",
    "        filtered_criticals.append(p_critical)\n",
    "        penalty = np.setdiff1d(p_critical, i_critical)\n",
    "        awarded = np.setdiff1d(i_critical, p_critical)\n",
    "        if current_penalty is None:\n",
    "            current_penalty = penalty\n",
    "        else:\n",
    "            current_penalty = np.union1d(current_penalty, penalty)\n",
    "        if current_awarded is None:\n",
    "            current_awarded = awarded\n",
    "        else:\n",
    "#                 current_awarded = np.intersect1d(current_awarded, awarded)\n",
    "            current_awarded = np.union1d(current_awarded, awarded)\n",
    "        print(\"current_penalty\", current_penalty, \"current_awarded\", current_awarded)\n",
    "        neurons.append((current_penalty, current_awarded))\n",
    "    neurons = neurons[1: (top_n + 1)]\n",
    "    return neurons\n",
    "\n",
    "\n",
    "path_dict = get_path_dict()\n",
    "model_path = \"models/adult_model.h5\"\n",
    "\n",
    "income_train_scores = get_relevance(model_path, pre_census_income.X_train,\n",
    "                                        save_path=os.path.join('scores/adult', os.path.basename(model_path) + \".score\"))\n",
    "income_critical = get_critical_neurons(income_train_scores, 0.3)\n",
    "finals = []\n",
    "\n",
    "top_n = 4\n",
    "protected_critical_ls = []\n",
    "\n",
    "a = 'g'\n",
    "path = path_dict[a][top_n - 1]\n",
    "train_scores = get_relevance(path, pre_census_income.X_train,  save_path=os.path.join('scores/adult', os.path.basename(path) + \".score\"))\n",
    "protected_critical = get_critical_neurons(train_scores, 0.3)\n",
    "protected_critical_ls.append(protected_critical)\n",
    "\n",
    "layer_num = len(income_critical)\n",
    "total_num = len(pre_census_income.X_train)\n",
    "neurons = get_penalty_awarded(top_n, layer_num, total_num, income_critical, protected_critical_ls)\n",
    "\n",
    "penalty = neurons[layer_index][0]\n",
    "awarded = neurons[layer_index][1]\n",
    "normal_neurons = [i for i in range(len(inter_output_adv[0])) if i not in neurons[layer_index][0] and i not in neurons[layer_index][1]]\n",
    "print((np.abs(inter_output_adv[:, penalty] - inter_output_ori[:, penalty])).sum() / len(penalty))\n",
    "print((np.abs(inter_output_adv[:, awarded] - inter_output_ori[:, awarded])).sum() / len(awarded))\n",
    "print((np.abs(inter_output_adv[:, normal_neurons] - inter_output_ori[:, normal_neurons])).sum() / len(normal_neurons))\n",
    "print(\"penalty\", penalty)\n",
    "print(\"awarded\", awarded)\n",
    "print(\"normal\", normal_neurons)\n",
    "\n",
    "print((np.abs(inter_output_adv - inter_output_ori)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 12)]              0         \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "scale_layer_5 (ScaleLayer)   (None, 30)                30        \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "scale_layer_6 (ScaleLayer)   (None, 20)                20        \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "scale_layer_7 (ScaleLayer)   (None, 15)                15        \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "scale_layer_8 (ScaleLayer)   (None, 15)                15        \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "layer6 (Dense)               (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,816\n",
      "Trainable params: 1,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "368.91714\n",
      "188515.02\n"
     ]
    }
   ],
   "source": [
    "model_path = \"models/diff_adult_g_gated_4_diff.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.2.h5\n",
    "model = keras.models.load_model(model_path, custom_objects={'ScaleLayer': ScaleLayer})\n",
    "model.summary()\n",
    "\n",
    "layer_name = layer_map[layer_index][1]\n",
    "inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "                                 \n",
    "layer_name = layer_map[layer_index][0]\n",
    "inter_model_before = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "                                 \n",
    "inter_output_ori = inter_model.predict(similar_X[0])\n",
    "inter_output_adv = inter_model.predict(similar_X[1])\n",
    "\n",
    "inter_output_ori_before = inter_model_before.predict(similar_X[0])\n",
    "inter_output_adv_before = inter_model_before.predict(similar_X[1])\n",
    "                                 \n",
    "print((np.abs(inter_output_adv - inter_output_ori)).sum())\n",
    "print((np.abs(inter_output_adv_before - inter_output_ori_before)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight [array([[ 7.5974339e-04, -5.3571258e-04, -5.2292697e-04, -8.3338644e-05,\n",
      "         1.9389287e-03,  2.4732639e-04, -4.9206632e-05,  2.5967529e-04,\n",
      "        -1.2399700e-01,  6.0464849e-04,  9.1479821e-03,  2.2583730e-04,\n",
      "         2.8091979e-03,  1.4539480e-03,  6.5577053e-04, -1.7852495e-04,\n",
      "         9.0948762e-03, -7.5216143e-05,  8.5201718e-05, -6.2989420e-04,\n",
      "         1.3095517e-02, -2.0224368e-04, -1.3083528e-03, -2.0436089e-02,\n",
      "         9.8191772e-04, -2.0736131e-04, -2.0158633e-03, -1.9681137e-03,\n",
      "        -8.8466989e-04,  4.5708522e-02]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "weight= model.get_layer('scale_layer_5').get_weights()\n",
    "print(\"weight\", weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_filter(layer_critical, total_num):\n",
    "    i_unique, i_counts = np.unique(layer_critical, return_counts=True)\n",
    "    i_rates = i_counts / total_num\n",
    "    i_sort = np.where(i_rates > 0.2)[0]  # np.argsort(i_counts*-1)\n",
    "    i_critical = i_unique[i_sort]\n",
    "    return i_critical\n",
    "\n",
    "def get_path_dict():\n",
    "    saved_model_path = \"models/finetuned_models_protected_attributes/adult/\"\n",
    "    path_ls = os.listdir(saved_model_path)\n",
    "    path_dict = {}\n",
    "    path_dict['r'] = [saved_model_path+p for p in path_ls if \"r_adult\" in p]\n",
    "    path_dict['g'] = [saved_model_path+p for p in path_ls if \"g_adult\" in p]\n",
    "    path_dict['a'] = [saved_model_path+p for p in path_ls if \"a_adult\" in p]\n",
    "    path_dict['r'].sort()\n",
    "    path_dict['g'].sort()\n",
    "    path_dict['a'].sort()\n",
    "    print(path_dict)\n",
    "    return path_dict\n",
    "\n",
    "def get_penalty_awarded(top_n, layer_num, total_num, income_critical, protected_critical_ls):\n",
    "    neurons = []\n",
    "\n",
    "    for i in range(layer_num):\n",
    "        income_layer_critical = income_critical[i].flatten()\n",
    "        i_critical = my_filter(income_layer_critical, total_num)\n",
    "        print(\"i_critical\", i_critical)\n",
    "        current_penalty = None\n",
    "        current_awarded = None\n",
    "        filtered_criticals = []\n",
    "        j = 0\n",
    "        a = 'g'\n",
    "        protected_layer_critical = protected_critical_ls[j][i].flatten()\n",
    "        p_critical = my_filter(protected_layer_critical, total_num)\n",
    "        print(\"p_critical\", p_critical)\n",
    "        filtered_criticals.append(p_critical)\n",
    "        penalty = np.setdiff1d(p_critical, i_critical)\n",
    "        awarded = np.setdiff1d(i_critical, p_critical)\n",
    "        if current_penalty is None:\n",
    "            current_penalty = penalty\n",
    "        else:\n",
    "            current_penalty = np.union1d(current_penalty, penalty)\n",
    "        if current_awarded is None:\n",
    "            current_awarded = awarded\n",
    "        else:\n",
    "#                 current_awarded = np.intersect1d(current_awarded, awarded)\n",
    "            current_awarded = np.union1d(current_awarded, awarded)\n",
    "        print(\"current_penalty\", current_penalty, \"current_awarded\", current_awarded)\n",
    "        neurons.append((current_penalty, current_awarded))\n",
    "    neurons = neurons[1: (top_n + 1)]\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': ['models/finetuned_models_protected_attributes/adult/r_adult_model_1_0.986.h5', 'models/finetuned_models_protected_attributes/adult/r_adult_model_2_0.946.h5', 'models/finetuned_models_protected_attributes/adult/r_adult_model_3_0.881.h5', 'models/finetuned_models_protected_attributes/adult/r_adult_model_4_0.871.h5', 'models/finetuned_models_protected_attributes/adult/r_adult_model_5_0.859.h5'], 'g': ['models/finetuned_models_protected_attributes/adult/g_adult_model_1_0.997.h5', 'models/finetuned_models_protected_attributes/adult/g_adult_model_2_0.968.h5', 'models/finetuned_models_protected_attributes/adult/g_adult_model_3_0.848.h5', 'models/finetuned_models_protected_attributes/adult/g_adult_model_4_0.826.h5', 'models/finetuned_models_protected_attributes/adult/g_adult_model_5_0.768.h5'], 'a': ['models/finetuned_models_protected_attributes/adult/a_adult_model_1_0.994.h5', 'models/finetuned_models_protected_attributes/adult/a_adult_model_2_0.965.h5', 'models/finetuned_models_protected_attributes/adult/a_adult_model_3_0.771.h5', 'models/finetuned_models_protected_attributes/adult/a_adult_model_4_0.705.h5', 'models/finetuned_models_protected_attributes/adult/a_adult_model_5_0.629.h5']}\n",
      "i_critical [ 2  8 10]\n",
      "p_critical [ 2  7  8  9 10]\n",
      "current_penalty [7 9] current_awarded []\n",
      "i_critical [ 7  9 10 14 15 16 19 20 22 28 29]\n",
      "p_critical [ 0  2  3  8  9 10 14 15 16 20 22 24 25 27 29]\n",
      "current_penalty [ 0  2  3  8 24 25 27] current_awarded [ 7 19 28]\n",
      "i_critical [ 0  2  3  5  6  9 12 13]\n",
      "p_critical [ 0  2  3  8  9 10 12 13 16 18]\n",
      "current_penalty [ 8 10 16 18] current_awarded [5 6]\n",
      "i_critical [ 1  5  7  8  9 10 12]\n",
      "p_critical [ 1  4  5  8 12]\n",
      "current_penalty [4] current_awarded [ 7  9 10]\n",
      "i_critical [ 1  4 10 14]\n",
      "p_critical [ 0  1  2  5  6  8 14]\n",
      "current_penalty [0 2 5 6 8] current_awarded [ 4 10]\n",
      "i_critical [1 2 3]\n",
      "p_critical [1 6 8 9]\n",
      "current_penalty [6 8 9] current_awarded [2 3]\n",
      "i_critical [0]\n",
      "p_critical [0]\n",
      "current_penalty [] current_awarded []\n",
      "i_critical [0]\n",
      "p_critical [0]\n",
      "current_penalty [] current_awarded []\n",
      "4.62468991960798\n",
      "1.3163425922393799\n",
      "16.62976989746094\n",
      "penalty [ 0  2  3  8 24 25 27]\n",
      "awarded [ 7 19 28]\n",
      "normal [1, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 26, 29]\n",
      "368.91714\n",
      "values [18152, 31258, 15933, 1934, 866, 0, 925, 29561, 11227, 20890, 2084, 0, 9117, 4931, 26667, 2939, 29646, 31112, 16416, 499, 701, 4023, 11977, 30030, 24653, 13093, 30061, 2620, 2807, 25711]\n",
      "values [19293, 31258, 17086, 1178, 495, 0, 2646, 28148, 11176, 24940, 2151, 0, 11449, 5449, 28666, 3807, 30057, 31184, 11340, 524, 837, 5430, 11060, 29997, 22811, 7076, 30456, 2445, 2770, 25468]\n"
     ]
    }
   ],
   "source": [
    "path_dict = get_path_dict()\n",
    "model_path = \"models/adult_model.h5\"\n",
    "\n",
    "income_train_scores = get_relevance(model_path, pre_census_income.X_train,\n",
    "                                        save_path=os.path.join('scores/adult', os.path.basename(model_path) + \".score\"))\n",
    "income_critical = get_critical_neurons(income_train_scores, 0.3)\n",
    "finals = []\n",
    "\n",
    "top_n = 4\n",
    "protected_critical_ls = []\n",
    "\n",
    "a = 'g'\n",
    "path = path_dict[a][top_n - 1]\n",
    "train_scores = get_relevance(path, pre_census_income.X_train,  save_path=os.path.join('scores/adult', os.path.basename(path) + \".score\"))\n",
    "protected_critical = get_critical_neurons(train_scores, 0.3)\n",
    "protected_critical_ls.append(protected_critical)\n",
    "\n",
    "layer_num = len(income_critical)\n",
    "total_num = len(pre_census_income.X_train)\n",
    "neurons = get_penalty_awarded(top_n, layer_num, total_num, income_critical, protected_critical_ls)\n",
    "\n",
    "penalty = neurons[layer_index][0]\n",
    "awarded = neurons[layer_index][1]\n",
    "normal_neurons = [i for i in range(len(inter_output_adv[0])) if i not in neurons[layer_index][0] and i not in neurons[layer_index][1]]\n",
    "print((np.abs(inter_output_adv[:, penalty] - inter_output_ori[:, penalty])).sum() / len(penalty))\n",
    "print((np.abs(inter_output_adv[:, awarded] - inter_output_ori[:, awarded])).sum() / len(awarded))\n",
    "print((np.abs(inter_output_adv[:, normal_neurons] - inter_output_ori[:, normal_neurons])).sum() / len(normal_neurons))\n",
    "print(\"penalty\", penalty)\n",
    "print(\"awarded\", awarded)\n",
    "print(\"normal\", normal_neurons)\n",
    "\n",
    "print((np.abs(inter_output_adv - inter_output_ori)).sum())\n",
    "\n",
    "# print(intermediate_output1 - intermediate_output0)\n",
    "\n",
    "s = [0 for i in range(len(inter_output_ori[0]))]\n",
    "for i in inter_output_ori:\n",
    "    for j in range(len(i)):\n",
    "        if i[j] > 0:   \n",
    "            s[j] += 1\n",
    "print(\"values\", s)\n",
    "\n",
    "s = [0 for i in range(len(inter_output_adv[0]))]\n",
    "for i in inter_output_adv:\n",
    "    for j in range(len(i)):\n",
    "        if i[j] > 0:   \n",
    "            s[j] += 1\n",
    "print(\"values\", s)\n",
    "# [0, 2, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.71061   ,  3.0536819 ,  2.024089  ,  0.58497125, 15.076021  ,\n",
       "        1.8527092 ,  0.569338  ,  1.9673985 , 17.34945   , 13.808918  ,\n",
       "       14.42024   ,  1.6813678 , 22.541544  ,  3.9344692 ,  7.0689754 ,\n",
       "        1.0338857 , 71.630424  ,  0.5526728 ,  1.275471  ,  0.97431767,\n",
       "       43.557945  ,  0.81608135,  3.742669  , 41.862415  ,  2.7926705 ,\n",
       "        2.3318763 , 23.685438  ,  4.577374  ,  1.0079285 , 60.42346   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.sum(np.abs(inter_output_adv - inter_output_ori), axis=0)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17,  6,  3, 21, 19, 28, 15, 18, 11,  5,  7,  2, 25,  0, 24,  1, 22,\n",
       "       13, 27, 14,  9, 10,  4,  8, 12, 26, 23, 20, 29, 16])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13762, 14828, 28289, 27386, 31148, 1354, 29, 26, 24738, 417, 28818, 26726, 1344, 949, 24587]\n"
     ]
    }
   ],
   "source": [
    "s = [0 for i in range(len(intermediate_output_ori_0[0]))]\n",
    "for i in intermediate_output_ori_0:\n",
    "    for j in range(len(i)):\n",
    "        if i[j] > 0:   \n",
    "            s[j] += 1\n",
    "print(s)\n",
    "# [0, 2, 5, 6, 8, 12]\n",
    "#  [4, 9 ,10, 13, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "layer6 (Dense)               (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,736\n",
      "Trainable params: 1,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "121393.95\n"
     ]
    }
   ],
   "source": [
    "model_path = \"models/adult_model.h5\"\n",
    "model = keras.models.load_model(model_path)\n",
    "model.summary()\n",
    "\n",
    "attr = 'g'\n",
    "protected_attribs = pos_map[attr]\n",
    "\n",
    "\n",
    "data_name = f\"data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "dis_data = np.load(data_name)\n",
    "num_attribs = len(dis_data[0])\n",
    "\n",
    "similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_census_income.constraint)\n",
    "layer_name = 'layer1'\n",
    "intermediate_layer_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "intermediate_output0 = intermediate_layer_model.predict(similar_X[0])\n",
    "intermediate_output1 = intermediate_layer_model.predict(similar_X[1])\n",
    "print((np.abs(intermediate_output1 - intermediate_output0)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** layer 6\n",
      "WARNING:tensorflow:From /home/xiaofei/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/xiaofei/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/xiaofei/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "[array([[ 2.9423175 ],\n",
      "       [-1.499268  ],\n",
      "       [-0.73832774],\n",
      "       [-0.76415855],\n",
      "       [ 0.704021  ],\n",
      "       [-4.6148725 ],\n",
      "       [-0.95385844],\n",
      "       [-0.68753624],\n",
      "       [-1.078599  ],\n",
      "       [ 1.7334886 ]], dtype=float32), array([-0.18941183], dtype=float32)]\n",
      "[-0.03226365  0.3509759   0.27754313  0.30573258 -0.22306842 -0.03183477\n",
      " -0.1574492   0.21742553  0.19734982 -0.06059706]\n",
      "attr: a 0.011\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "[array([[ 1.0153329 ],\n",
      "       [-0.9641135 ],\n",
      "       [-0.31642342],\n",
      "       [-0.5854113 ],\n",
      "       [ 0.05338898],\n",
      "       [ 0.6885379 ],\n",
      "       [-0.5771605 ],\n",
      "       [-0.10637736],\n",
      "       [-0.07460939],\n",
      "       [ 0.39883998]], dtype=float32), array([-0.7370341], dtype=float32)]\n",
      "[ 0.08277094 -4.2889376  -4.362182    1.8577232  -2.2199702  -6.7334766\n",
      "  3.772274   -2.2696352   0.15300778  2.556024  ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6fa7ecb3d09b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0minter_output_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minter_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minter_output_adv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minter_output_ori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_v\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_census_income\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "from scalelayer import  ScaleLayer\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            'r': [6],\n",
    "            'g': [7],\n",
    "            'a&r': [0, 6],\n",
    "            'a&g': [0, 7],\n",
    "            'r&g': [6, 7]\n",
    "            }\n",
    "\n",
    "models_map = {\n",
    "    'a': \"models/gated_models/adult_a_gated_4_0.3_0.2_p-0.3_p0.15.h5\",\n",
    "    'r': \"models/gated_models/adult_r_gated_4_0.3_0.2_p-0.95_p0.8.h5\",\n",
    "    'g': \"models/gated_models/adult_g_gated_4_0.3_0.2_p-0.6_p0.1.h5\",\n",
    "    'a&r': \"models/gated_models/adult_a&r_gated_4_0.3_0.2_p-0.35_p0.25.h5\",\n",
    "    'a&g': \"models/gated_models/adult_a&g_gated_4_0.3_0.2_p-0.3_p0.25.h5\",\n",
    "    'r&g': \"models/gated_models/adult_r&g_gated_4_0.3_0.2_p-0.9_p0.8.h5\",\n",
    "}\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('layer1', 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'layer5'), ('layer5', 'layer6')]\n",
    "\n",
    "from preprocessing import pre_census_income\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for layer_index in range(5, 6):\n",
    "    print(\"*\"*10, \"layer\", layer_index+1)\n",
    "    for attr in pos_map.keys():\n",
    "        protected_attribs = pos_map[attr]\n",
    "        \n",
    "        model_path = models_map[attr]\n",
    "        # adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "        model = keras.models.load_model(model_path, custom_objects={'ScaleLayer': ScaleLayer})\n",
    "        print(model.get_layer('layer6').get_weights())\n",
    "#         model.summary()\n",
    "\n",
    "        # data_name = f\"data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "        # dis_data = np.load(data_name)\n",
    "\n",
    "        dis_data = pre_census_income.X_train\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_census_income.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index-1][1]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(pre_census_income.X_train)\n",
    "        \n",
    "        max_v = inter_output_ori.max()\n",
    "        min_v = inter_output_ori.min()\n",
    "        print(np.sum(inter_output_ori), axis=1)\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum() / (max_v - min_v)\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
