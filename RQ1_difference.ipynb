{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ebf9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xiaofei/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/xiaofei/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/xiaofei/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "layer6 (Dense)               (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,736\n",
      "Trainable params: 1,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_census_income\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=pre_census_income.X_train.shape[1:]),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            'r': [6],\n",
    "            'g': [7],\n",
    "            'a&r': [0, 6],\n",
    "            'a&g': [0, 7],\n",
    "            'r&g': [6, 7]\n",
    "            }\n",
    "\n",
    "model_path = \"models/adult_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('layer1', 'scale_layer_5'), ('layer2', 'scale_layer_6'), ('layer3', 'scale_layer_7'), ('layer4', 'scale_layer_8'), ('layer5', 'scale_layer_8'), ('layer6', 'scale_layer_8')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99695cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0\n",
      "attr: a 4.131\n",
      "attr: r 4.439\n",
      "attr: g 1.586\n",
      "attr: a&r 6.882\n",
      "attr: a&g 5.067\n",
      "attr: r&g 5.119\n",
      "layer 1\n",
      "attr: a 3.017\n",
      "attr: r 1.722\n",
      "attr: g 1.162\n",
      "attr: a&r 3.88\n",
      "attr: a&g 3.585\n",
      "attr: r&g 2.39\n",
      "layer 2\n",
      "attr: a 1.104\n",
      "attr: r 0.674\n",
      "attr: g 0.42\n",
      "attr: a&r 1.471\n",
      "attr: a&g 1.295\n",
      "attr: r&g 0.943\n",
      "layer 3\n",
      "attr: a 1.688\n",
      "attr: r 0.906\n",
      "attr: g 0.612\n",
      "attr: a&r 2.166\n",
      "attr: a&g 1.925\n",
      "attr: r&g 1.291\n",
      "layer 4\n",
      "attr: a 1.902\n",
      "attr: r 0.921\n",
      "attr: g 0.637\n",
      "attr: a&r 2.365\n",
      "attr: a&g 2.161\n",
      "attr: r&g 1.324\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import pre_census_income\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for layer_index in range(5):\n",
    "    print(\"layer\", layer_index)\n",
    "    for attr in pos_map.keys():\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        # data_name = f\"data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "        # dis_data = np.load(data_name)\n",
    "\n",
    "        dis_data = pre_census_income.X_train\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_census_income.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(pre_census_income.X_train)\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum()\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, round(diff/num, 3))\n",
    "                                 \n",
    "# print((np.abs(inter_output_adv - inter_output_ori)).sum())\n",
    "# print((np.abs(inter_output_adv_before - inter_output_ori_before)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a8167d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_132 (Dense)            (None, 30)                510       \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,856\n",
      "Trainable params: 1,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 0\n",
      "attr: a 2.297\n",
      "layer 1\n",
      "attr: a 1.813\n",
      "layer 2\n",
      "attr: a 1.074\n",
      "layer 3\n",
      "attr: a 1.055\n",
      "layer 4\n",
      "attr: a 1.07\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_bank_marketing\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=pre_bank_marketing.X_train.shape[1:]),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            }\n",
    "\n",
    "model_path = \"models/bank_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('dense_132', 'scale_layer_5'), ('dense_133', 'scale_layer_6'), ('dense_134', 'scale_layer_7'), ('dense_135', 'scale_layer_8'), ('dense_136', 'scale_layer_8'), ('dense_137', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_bank_marketing\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for layer_index in range(5):\n",
    "    print(\"layer\", layer_index)\n",
    "    for attr in pos_map.keys():\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        # data_name = f\"data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "        # dis_data = np.load(data_name)\n",
    "\n",
    "        dis_data = pre_bank_marketing.X_train\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_bank_marketing.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(pre_bank_marketing.X_train)\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum()\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d445c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,736\n",
      "Trainable params: 1,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 0\n",
      "attr: a 2.839\n",
      "attr: r 3.271\n",
      "attr: g 0.91\n",
      "attr: a&r 4.653\n",
      "attr: a&g 3.253\n",
      "attr: r&g 3.583\n",
      "layer 1\n",
      "attr: a 2.122\n",
      "attr: r 2.835\n",
      "attr: g 0.833\n",
      "attr: a&r 3.838\n",
      "attr: a&g 2.504\n",
      "attr: r&g 3.146\n",
      "layer 2\n",
      "attr: a 2.711\n",
      "attr: r 2.775\n",
      "attr: g 0.775\n",
      "attr: a&r 4.227\n",
      "attr: a&g 3.023\n",
      "attr: r&g 3.068\n",
      "layer 3\n",
      "attr: a 1.52\n",
      "attr: r 1.513\n",
      "attr: g 0.43\n",
      "attr: a&r 2.293\n",
      "attr: a&g 1.675\n",
      "attr: r&g 1.671\n",
      "layer 4\n",
      "attr: a 0.03\n",
      "attr: r 0.036\n",
      "attr: g 0.009\n",
      "attr: a&r 0.049\n",
      "attr: a&g 0.033\n",
      "attr: r&g 0.039\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_census_income\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=pre_census_income.X_train.shape[1:]),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            'r': [6],\n",
    "            'g': [7],\n",
    "            'a&r': [0, 6],\n",
    "            'a&g': [0, 7],\n",
    "            'r&g': [6, 7]\n",
    "            }\n",
    "\n",
    "model_path = \"models/retrained_models_EIDIG/adult_EIDIG_INF_retrained_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('dense_1', 'scale_layer_5'), ('dense_2', 'scale_layer_6'), ('dense_3', 'scale_layer_7'), ('dense_4', 'scale_layer_8'), ('dense_5', 'scale_layer_8'), ('dense_6', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_census_income\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for layer_index in range(5):\n",
    "    print(\"layer\", layer_index)\n",
    "    for attr in pos_map.keys():\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        # data_name = f\"data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "        # dis_data = np.load(data_name)\n",
    "\n",
    "        dis_data = pre_census_income.X_train\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_census_income.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(pre_census_income.X_train)\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum()\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, round(diff/num, 3))\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "061c5fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 30)                510       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 1,666\n",
      "Trainable params: 1,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 0\n",
      "attr: a 2.255\n",
      "layer 1\n",
      "attr: a 1.858\n",
      "layer 2\n",
      "attr: a 1.081\n",
      "layer 3\n",
      "attr: a 0.806\n",
      "layer 4\n",
      "attr: a 0.661\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_bank_marketing\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=pre_bank_marketing.X_train.shape[1:]),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            }\n",
    "\n",
    "model_path = \"models/bank_EIDIG_INF_retrained_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('dense_12', 'scale_layer_5'), ('dense_13', 'scale_layer_6'), ('dense_14', 'scale_layer_7'), ('dense_15', 'scale_layer_8'), ('dense_16', 'scale_layer_8'), ('dense_17', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_bank_marketing\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for layer_index in range(5):\n",
    "    print(\"layer\", layer_index)\n",
    "    for attr in pos_map.keys():\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        # data_name = f\"data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "        # dis_data = np.load(data_name)\n",
    "\n",
    "        dis_data = pre_bank_marketing.X_train\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_bank_marketing.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(pre_bank_marketing.X_train)\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum()\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92fc285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xiaofei/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/xiaofei/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/xiaofei/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                600       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 2,816\n",
      "Trainable params: 2,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 0\n",
      "attr: r 3.035\n",
      "attr: g 1.034\n",
      "attr: g&r 3.477\n",
      "layer 1\n",
      "attr: r 1.718\n",
      "attr: g 0.637\n",
      "attr: g&r 1.98\n",
      "layer 2\n",
      "attr: r 1.155\n",
      "attr: g 0.577\n",
      "attr: g&r 1.413\n",
      "layer 3\n",
      "attr: r 0.679\n",
      "attr: g 0.284\n",
      "attr: g&r 0.798\n",
      "layer 4\n",
      "attr: r 0.041\n",
      "attr: g 0.021\n",
      "attr: g&r 0.05\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_lsac\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(50, activation=\"relu\", input_shape=pre_lsac.X_train.shape[1:]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(5, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(int(constraint[i][0]), int(constraint[i][1]+1)))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { \n",
    "            'r': [10],\n",
    "            'g': [9],\n",
    "            'g&r': [10, 9]\n",
    "            }\n",
    "\n",
    "model_path = \"models/lsac_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = keras.models.load_model(model_path)\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('dense_1', 'scale_layer_5'), ('dense_2', 'scale_layer_6'), ('dense_3', 'scale_layer_7'), ('dense_4', 'scale_layer_8'), ('dense_5', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_lsac\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for layer_index in range(5):\n",
    "    print(\"layer\", layer_index)\n",
    "    for attr in pos_map.keys():\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        # data_name = f\"data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "        # dis_data = np.load(data_name)\n",
    "\n",
    "        dis_data = pre_lsac.X_train\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_lsac.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(pre_lsac.X_train)\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum()\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xiaofei/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 50)                600       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 2,816\n",
      "Trainable params: 2,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "layer 0\n",
      "attr: r 2.959\n",
      "attr: g 1.464\n",
      "attr: g&r 3.711\n",
      "layer 1\n",
      "attr: r 1.714\n",
      "attr: g 0.838\n",
      "attr: g&r 2.125\n",
      "layer 2\n",
      "attr: r 0.697\n",
      "attr: g 0.315\n",
      "attr: g&r 0.856\n",
      "layer 3\n",
      "attr: r 0.573\n",
      "attr: g 0.273\n",
      "attr: g&r 0.707\n",
      "layer 4\n",
      "attr: r 0.524\n",
      "attr: g 0.26\n",
      "attr: g&r 0.649\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_lsac\n",
    "import tensorflow.keras.backend as K\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "def my_loss_fun(y_true, y_pred):\n",
    "    # do whatever you want\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(50, activation=\"relu\", input_shape=pre_lsac.X_train.shape[1:]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(15, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(5, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(int(constraint[i][0]), int(constraint[i][1]+1)))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "pos_map = { \n",
    "            'r': [10],\n",
    "            'g': [9],\n",
    "            'g&r': [10, 9]\n",
    "            }\n",
    "\n",
    "model_path = \"models/retrained_models_EIDIG/lsac_EIDIG_INF_retrained_model.h5\"\n",
    "# adult_g_gated_4_0.3_0.2_p-0.1_p0.9.h5\n",
    "model = tf.keras.models.load_model(model_path, compile=False)\n",
    "model.summary()\n",
    "\n",
    "# similar_X = [dis_data, new_data]\n",
    "\n",
    "layer_map = [('dense_18', 'scale_layer_5'), ('dense_19', 'scale_layer_6'), ('dense_20', 'scale_layer_7'), ('dense_21', 'scale_layer_8'), ('dense_22', 'scale_layer_8')]\n",
    "\n",
    "from preprocessing import pre_lsac\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for layer_index in range(5):\n",
    "    print(\"layer\", layer_index)\n",
    "    for attr in pos_map.keys():\n",
    "        protected_attribs = pos_map[attr]\n",
    "\n",
    "        # data_name = f\"data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "        # dis_data = np.load(data_name)\n",
    "\n",
    "        dis_data = pre_lsac.X_train\n",
    "        num_attribs = len(dis_data[0])\n",
    "        new_data = dis_data.copy()\n",
    "\n",
    "        similar_X = similar_set(dis_data, num_attribs, protected_attribs, pre_lsac.constraint)\n",
    "\n",
    "        # layer_name = layer_map[layer_index][1]\n",
    "        # inter_model = Model(model.input, model.get_layer(layer_name).output)\n",
    "\n",
    "        layer_name = layer_map[layer_index][0]\n",
    "        inter_model = Model(model.input, model.get_layer(layer_name).output)                                 \n",
    "\n",
    "        inter_output_ori = inter_model.predict(pre_lsac.X_train)\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(len(similar_X)):\n",
    "            inter_output_adv = inter_model.predict(similar_X[i])\n",
    "            diff += np.abs(inter_output_adv - inter_output_ori).sum()\n",
    "\n",
    "        num = len(similar_X) * similar_X[0].shape[0]\n",
    "        print(\"attr:\", attr, round(diff/num, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebf6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
