{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3789376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from explain import  get_relevance, get_critical_neurons\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "from scalelayer import  ScaleLayer\n",
    "from numpy.random import seed\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import pre_census_income\n",
    "import tensorflow.keras.backend as KTF\n",
    "import argparse\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_random_seed(2)\n",
    "\n",
    "\n",
    "def similar_set(X, num_attribs, protected_attribs, constraint):\n",
    "    # find all similar inputs corresponding to different combinations of protected attributes with non-protected attributes unchanged\n",
    "    similar_X = []\n",
    "    protected_domain = []\n",
    "    for i in protected_attribs:\n",
    "        protected_domain = protected_domain + [list(range(constraint[i][0], constraint[i][1]+1))]\n",
    "    all_combs = np.array(list(itertools.product(*protected_domain)))\n",
    "    for i, comb in enumerate(all_combs):\n",
    "        X_new = copy.deepcopy(X)\n",
    "        for a, c in zip(protected_attribs, comb):\n",
    "            X_new[:, a] = c\n",
    "        similar_X.append(X_new)\n",
    "    return similar_X\n",
    "\n",
    "def get_repaired_num(newdata_res):\n",
    "    # identify whether the instance is discriminatory w.r.t. the model\n",
    "    # print(x.shape)\n",
    "    # print(X_train[0].shape)\n",
    "    # y_pred = (model(tf.constant([X_train[0]])) > 0.5)\n",
    "    l = len(newdata_res)\n",
    "    for i in range(l-1):\n",
    "        tmp_acc = (newdata_res[i] == newdata_res[i + 1]) * 1\n",
    "        if i == 0:\n",
    "            acc = tmp_acc\n",
    "        else:\n",
    "            acc += tmp_acc\n",
    "    return np.sum(np.where(acc == l-1, True, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b2ac69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr a\n",
      "WARNING:tensorflow:From /home/xiaofei/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/xiaofei/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/xiaofei/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Aug 0.4348434156697388\n",
      "attr r\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Aug 0.8528403018169637\n",
      "attr g\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Aug 0.7104122329944037\n",
      "attr a&r\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Aug 0.3693649641465481\n",
      "attr a&g\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Aug 0.3473111053890937\n",
      "attr r&g\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Aug 0.38234888078098805\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val, constraint = pre_census_income.X_train, \\\n",
    "    pre_census_income.X_val, pre_census_income.y_train, pre_census_income.y_val, pre_census_income.constraint\n",
    "    \n",
    "X_test, y_test = pre_census_income.X_test, pre_census_income.y_test\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            'r': [6],\n",
    "            'g': [7],\n",
    "            'a&r': [0, 6],\n",
    "            'a&g': [0, 7],\n",
    "            'r&g': [6, 7]\n",
    "            }\n",
    "\n",
    "models_map = { 'a': \"models/retrained_adv/a_adult_model_0.h5\",\n",
    "            'r': \"models/retrained_adv/r_adult_model_0.h5\",\n",
    "            'g': \"models/retrained_adv/g_adult_model_0.h5\",\n",
    "            'a&r': \"models/retrained_adv/a&r_adult_model_0.h5\",\n",
    "            'a&g': \"models/retrained_adv/a&g_adult_model_0.h5\",\n",
    "            'r&g': \"models/retrained_adv/r&g_adult_model_0.h5\",\n",
    "    \n",
    "}\n",
    "\n",
    "for attr in pos_map.keys():\n",
    "    print(\"attr\", attr)\n",
    "    target_model_path = models_map[attr]\n",
    "    data_name = f\"discriminatory_data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "    dis_data = np.load(data_name)\n",
    "    num_attribs = len(X_train[0])\n",
    "    protected_attribs = pos_map[attr]\n",
    "    similar_X = similar_set(dis_data, num_attribs, protected_attribs, constraint)\n",
    "\n",
    "    augmented_model = keras.models.load_model(target_model_path)\n",
    "    aug_data = (augmented_model.predict(dis_data) > 0.5).astype(int).flatten()\n",
    "    dis_num = 0\n",
    "    newdata_res = []\n",
    "\n",
    "    l = len(similar_X)\n",
    "    for i in range(l):\n",
    "        newdata_re = augmented_model.predict(similar_X[i])\n",
    "        newdata_re = (newdata_re > 0.5).astype(int).flatten()\n",
    "        newdata_res.append(newdata_re)\n",
    "    repaired_num = get_repaired_num(newdata_res)\n",
    "\n",
    "    print('Aug', repaired_num/len(dis_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba2c58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr a\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sample_weight_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0fc256ba8c90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0msimilar_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilar_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_attribs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotected_attribs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0maugmented_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0maug_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maugmented_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdis_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    141\u001b[0m   if (h5py is not None and (\n\u001b[1;32m    142\u001b[0m       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;31m# Compile model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       model.compile(**saving_utils.compile_args_from_training_config(\n\u001b[0;32m--> 178\u001b[0;31m           training_config, custom_objects))\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0;31m# Set optimizer weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/debias/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mcompile_args_from_training_config\u001b[0;34m(training_config, custom_objects)\u001b[0m\n\u001b[1;32m    228\u001b[0m       \u001b[0;32mlambda\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m       training_config.get('weighted_metrics', None))\n\u001b[0;32m--> 230\u001b[0;31m   \u001b[0msample_weight_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight_mode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m   \u001b[0mloss_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sample_weight_mode'"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val, constraint = pre_census_income.X_train, \\\n",
    "    pre_census_income.X_val, pre_census_income.y_train, pre_census_income.y_val, pre_census_income.constraint\n",
    "    \n",
    "X_test, y_test = pre_census_income.X_test, pre_census_income.y_test\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            'r': [6],\n",
    "            'g': [7],\n",
    "            'a&r': [0, 6],\n",
    "            'a&g': [0, 7],\n",
    "            'r&g': [6, 7]\n",
    "            }\n",
    "\n",
    "models_map = { 'a': \"models/models_my_attribute_wise_retrain/adult_a_EIDIG_INF_retrained_model.h5\",\n",
    "            'r': \"models/models_my_attribute_wise_retrain/adult_r_EIDIG_INF_retrained_model.h5\",\n",
    "            'g': \"models/models_my_attribute_wise_retrain/adult_g_EIDIG_INF_retrained_model.h5\",\n",
    "            'a&r': \"models/models_my_attribute_wise_retrain/adult_a&r_EIDIG_INF_retrained_model.h5\",\n",
    "            'a&g': \"models/models_my_attribute_wise_retrain/adult_a&g_EIDIG_INF_retrained_model.h5\",\n",
    "            'r&g': \"models/models_my_attribute_wise_retrain/adult_r&g_EIDIG_INF_retrained_model.h5\",\n",
    "    \n",
    "}\n",
    "\n",
    "for attr in pos_map.keys():\n",
    "    print(\"attr\", attr)\n",
    "    target_model_path = models_map[attr]\n",
    "    data_name = f\"discriminatory_data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "    dis_data = np.load(data_name)\n",
    "    num_attribs = len(X_train[0])\n",
    "    protected_attribs = pos_map[attr]\n",
    "    similar_X = similar_set(dis_data, num_attribs, protected_attribs, constraint)\n",
    "\n",
    "    augmented_model = keras.models.load_model(target_model_path)\n",
    "    aug_data = (augmented_model.predict(dis_data) > 0.5).astype(int).flatten()\n",
    "    dis_num = 0\n",
    "    newdata_res = []\n",
    "\n",
    "    l = len(similar_X)\n",
    "    for i in range(l):\n",
    "        newdata_re = augmented_model.predict(similar_X[i])\n",
    "        newdata_re = (newdata_re > 0.5).astype(int).flatten()\n",
    "        newdata_res.append(newdata_re)\n",
    "    repaired_num = get_repaired_num(newdata_res)\n",
    "\n",
    "    print('Aug', repaired_num/len(dis_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, constraint = pre_census_income.X_train, \\\n",
    "    pre_census_income.X_val, pre_census_income.y_train, pre_census_income.y_val, pre_census_income.constraint\n",
    "    \n",
    "X_test, y_test = pre_census_income.X_test, pre_census_income.y_test\n",
    "\n",
    "pos_map = { 'a': [0],\n",
    "            'r': [6],\n",
    "            'g': [7],\n",
    "            'a&r': [0, 6],\n",
    "            'a&g': [0, 7],\n",
    "            'r&g': [6, 7]\n",
    "            }\n",
    "\n",
    "models_map = { 'a': \"models/models_flipped_retrain/adult_a_flipped_retrained_model.h5\",\n",
    "            'r': \"models/models_flipped_retrain/adult_r_flipped_retrained_model.h5,,\n",
    "            'g': \"models/models_flipped_retrain/adult_g_flipped_retrained_model.h5\",\n",
    "            'a&r': \"models/models_flipped_retrain/adult_a&r_flipped_retrained_model.h5\",\n",
    "            'a&g': \"models/models_flipped_retrain/adult_a&g_flipped_retrained_model.h5\",\n",
    "            'r&g': \"models/models_flipped_retrain/adult_r&g_flipped_retrained_model.h5\",\n",
    "    \n",
    "}\n",
    "\n",
    "for attr in pos_map.keys():\n",
    "    print(\"attr\", attr)\n",
    "    target_model_path = models_map[attr]\n",
    "    data_name = f\"discriminatory_data/adult/C-{attr}_ids_EIDIG_INF.npy\"\n",
    "    dis_data = np.load(data_name)\n",
    "    num_attribs = len(X_train[0])\n",
    "    protected_attribs = pos_map[attr]\n",
    "    similar_X = similar_set(dis_data, num_attribs, protected_attribs, constraint)\n",
    "\n",
    "    augmented_model = keras.models.load_model(target_model_path)\n",
    "    aug_data = (augmented_model.predict(dis_data) > 0.5).astype(int).flatten()\n",
    "    dis_num = 0\n",
    "    newdata_res = []\n",
    "\n",
    "    l = len(similar_X)\n",
    "    for i in range(l):\n",
    "        newdata_re = augmented_model.predict(similar_X[i])\n",
    "        newdata_re = (newdata_re > 0.5).astype(int).flatten()\n",
    "        newdata_res.append(newdata_re)\n",
    "    repaired_num = get_repaired_num(newdata_res)\n",
    "\n",
    "    print('Aug', repaired_num/len(dis_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74901c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
